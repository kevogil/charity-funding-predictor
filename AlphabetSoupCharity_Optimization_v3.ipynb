{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlphabetSoupCharity_Optimization_v3",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hloFxNfLb_Xx"
      },
      "source": [
        "# Install kerastuner\n",
        "!pip install keras-tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ba6eLNkcKGM"
      },
      "source": [
        "# Import our dependencies\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQnQtRFFJs9n"
      },
      "source": [
        "#  Import and read the charity_data.csv.\n",
        "url = \"https://raw.githubusercontent.com/kevogil/charity-funding-predictor/main/Resources/charity_data.csv\"\n",
        "application_df = pd.read_csv(url)\n",
        "\n",
        "# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.\n",
        "application_df.drop(columns=['EIN', 'NAME', 'ORGANIZATION', 'SPECIAL_CONSIDERATIONS'], inplace=True)\n",
        "\n",
        "# Look at APPLICATION_TYPE value counts for binning\n",
        "app_cnt = application_df['APPLICATION_TYPE'].value_counts()\n",
        "\n",
        "# Choose a cutoff value and create a list of application types to be replaced\n",
        "# use the variable name `application_types_to_replace`\n",
        "cutoff_point = 500\n",
        "application_types_to_replace = app_cnt[app_cnt < cutoff_point].index.tolist()\n",
        "\n",
        "\n",
        "# Replace in dataframe\n",
        "for app in application_types_to_replace:\n",
        "    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,\"Other\")\n",
        "\n",
        "# Look at CLASSIFICATION value counts for binning\n",
        "class_cnt = application_df['CLASSIFICATION'].value_counts()\n",
        "\n",
        "# Choose a cutoff value and create a list of classifications to be replaced\n",
        "# use the variable name `classifications_to_replace`\n",
        "cutoff_point = 1000\n",
        "classifications_to_replace = class_cnt[class_cnt < cutoff_point].index.tolist()\n",
        "\n",
        "# Replace in dataframe\n",
        "for cls in classifications_to_replace:\n",
        "    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,\"Other\")\n",
        "    \n",
        "# Convert categorical data to numeric with `pd.get_dummies`\n",
        "numeric_application_df = pd.get_dummies(application_df)\n",
        "\n",
        "# Split our preprocessed data into our features and target arrays\n",
        "y = numeric_application_df['IS_SUCCESSFUL']\n",
        "x = numeric_application_df.drop(columns=['IS_SUCCESSFUL'])\n",
        "\n",
        "# Split the preprocessed data into a training and testing dataset\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
        "\n",
        "# Create a StandardScaler instances\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler\n",
        "x_scaler = scaler.fit(x_train)\n",
        "\n",
        "# Scale the data\n",
        "x_train_scaled = x_scaler.transform(x_train)\n",
        "x_test_scaled = x_scaler.transform(x_test)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQfWymQp5h15"
      },
      "source": [
        "### Compile, Train and Evaluate the Model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKY6E47Dcmwj"
      },
      "source": [
        "# Create a method that creates a new Sequential model with hyperparameter options\n",
        "def create_model(hp):\n",
        "    nn_model = tf.keras.models.Sequential()\n",
        "\n",
        "    # Allow kerastuner to decide which activation function to use in hidden layers\n",
        "    activation = hp.Choice('activation',['relu','tanh','sigmoid'])\n",
        "    \n",
        "    # Allow kerastuner to decide number of neurons in first layer\n",
        "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n",
        "        min_value=1,\n",
        "        max_value=10,\n",
        "        step=2), activation=activation, input_dim=len(x_train_scaled[0])))\n",
        "\n",
        "    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n",
        "    for i in range(hp.Int('num_layers', 1, 6)):\n",
        "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
        "            min_value=1,\n",
        "            max_value=10,\n",
        "            step=2),\n",
        "            activation=activation))\n",
        "    \n",
        "    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "    # Compile the model\n",
        "    nn_model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
        "    \n",
        "    return nn_model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPgpc8GxdD78",
        "outputId": "99dfd337-e4dc-4758-8b01-ae1283ac0f4d"
      },
      "source": [
        "# Import the kerastuner library\n",
        "import kerastuner as kt\n",
        "\n",
        "tuner = kt.Hyperband(\n",
        "    create_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_epochs=20,\n",
        "    hyperband_iterations=2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Reloading Oracle from existing project ./untitled_project/oracle.json\n",
            "INFO:tensorflow:Reloading Tuner from ./untitled_project/tuner0.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpntqTLsdEM4",
        "outputId": "767d7fda-3d9d-4679-f095-35ebf77512e7"
      },
      "source": [
        "# Run the kerastuner search for best hyperparameters\n",
        "tuner.search(x_train_scaled,y_train,epochs=20,validation_data=(x_test_scaled,y_test))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 61 Complete [00h 00m 30s]\n",
            "val_accuracy: 0.7248979806900024\n",
            "\n",
            "Best val_accuracy So Far: 0.7271137237548828\n",
            "Total elapsed time: 00h 01m 02s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIcJIwE4dERa",
        "outputId": "e145826b-2ff7-44d2-ea9a-4c4a44b32599"
      },
      "source": [
        "# Get best model hyperparameters\n",
        "best_hyper = tuner.get_best_hyperparameters(3)[0]\n",
        "best_hyper.values"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'tanh',\n",
              " 'first_units': 5,\n",
              " 'num_layers': 2,\n",
              " 'tuner/bracket': 1,\n",
              " 'tuner/epochs': 20,\n",
              " 'tuner/initial_epoch': 7,\n",
              " 'tuner/round': 1,\n",
              " 'tuner/trial_id': 'df65570c8935180e15bed8bd93f1df2f',\n",
              " 'units_0': 5,\n",
              " 'units_1': 9,\n",
              " 'units_2': 3,\n",
              " 'units_3': 9,\n",
              " 'units_4': 7}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiSXINMEdEUZ",
        "outputId": "abe2a978-02e7-4f15-bc35-e307256b335e"
      },
      "source": [
        "# Evaluate best model against full test data\n",
        "best_model = tuner.get_best_models(3)[0]\n",
        "model_loss, model_accuracy = best_model.evaluate(x_test_scaled,y_test,verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "268/268 - 0s - loss: 0.5659 - accuracy: 0.7271 - 404ms/epoch - 2ms/step\n",
            "Loss: 0.5658575892448425, Accuracy: 0.7271137237548828\n"
          ]
        }
      ]
    }
  ]
}